{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it30o3cgHQfv"
      },
      "source": [
        "# Neural Rendering with Attention: An Incremental Improvement for Anime Character Animation\n",
        "![image](https://transpchan.github.io/live3d/main.png)\n",
        "This notebook is the official demo of [Neural Rendering with Attention: An Incremental Improvement for Anime Character Animation](https://github.com/transpchan/Live3D-v2). \n",
        "Pressing Ctrl+F9 will run all the code in sequence, and have fun!\n",
        "\n",
        "本笔记本是Neural Rendering with Attention: An Incremental Improvement for Anime Character Animation 的官方demo。\n",
        "按Ctrl+F9将依次运行全部的代码，玩得开心！\n",
        "\n",
        "このノートブックは、Neural Rendering with Attention: An Incremental Improvement for Anime Character Animation の公式デモです。\n",
        "Ctrl+F9 を押すと、すべてのコードが順番に実行され、楽しくなります!\n",
        "\n",
        "이 노트북은 Neural Rendering with Attention: An Incremental Improvement for Anime Character Animation 의 공식 데모입니다.\n",
        "Ctrl+F9를 누르면 모든 코드가 순서대로 실행되며 재미있습니다!\n",
        "\n",
        "Dieses Notebook ist die offizielle Demo von Neural Rendering with Attention: An Incremental Improvement for Anime Character Animation.\n",
        "Durch Drücken von Strg+F9 wird der gesamte Code nacheinander ausgeführt, und viel Spaß!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6v8hfgi_OT5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download the code\n",
        "#@markdown Download the code from https://github.com/transpchan/Live3D-v2 and install requirements.\n",
        "!git clone https://github.com/transpchan/Live3D-v2.git\n",
        "%cd Live3D-v2\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bwsHqZvzCAka"
      },
      "outputs": [],
      "source": [
        "#@title Download weights\n",
        "#@markdown You may also replace the link to the latest weight on https://github.com/transpchan/Live3D-v2/releases.\n",
        "\n",
        "!mkdir weights\n",
        "#!curl -O -J -L  https://github.com/transpchan/Live3D-v2/releases/download/checkpoints-2.1/checkpoints.zip\n",
        "!curl -O -J -L  https://github.com/transpchan/Live3D-v2/releases/download/checkpoints-2.2/checkpoints.zip\n",
        "!unzip checkpoints.zip -d ./weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rxauWls8Ap4N"
      },
      "outputs": [],
      "source": [
        "#@title Choose a Sample. \n",
        "#@markdown Choose sample character sheets and UDP sequences. \n",
        "\n",
        "\n",
        "\n",
        "character = 'self_defined' #@param ['double_ponytail', 'short_hair', 'self_defined']\n",
        "!mkdir -p character_sheet/character\n",
        "\n",
        "#@markdown **Caution**: The character sheets should be **PNG files with transparent background** and the character in them should be as similar as possible to the one given in the UDP sequence. Please you can try using https://github.com/KurisuMakise004/MMD2UDP (notebook:https://colab.research.google.com/github/KurisuMakise004/MMD2UDP/blob/main/COLAB.ipynb) to generate your own UDP sequence.\n",
        "\n",
        "\n",
        "#@markdown **注意**：设定集必须是**透明背景的PNG文件**，里面的人物尽量和UDP序列中给出的人物相似。 请您尝试使用 MMD2UDP （ 笔记本：https://colab.research.google.com/github/KurisuMakise004/MMD2UDP/blob/main/COLAB.ipynb ）生成您自己的 UDP 序列。\n",
        "\n",
        "#@markdown **注意**: キャラクター シートは、**背景が透明な PNG ファイル**である必要があり、その中の人物は、UDP シーケンスで指定されたものにできるだけ似ている必要があります。 MMD2UDP を使用して、独自の UDP シーケンスを生成してみてください。\n",
        "\n",
        "#@markdown **주의**: 캐릭터 시트는 **배경이 투명한 PNG 파일**이어야 하며 인물은 UDP 시퀀스에서 주어진 것과 최대한 유사해야 합니다. MMD2UDP 를 사용하여 고유한 UDP 시퀀스를 생성할 수 있습니다.\n",
        "\n",
        "#@markdown **Achtung**: Die Charakterbögen sollten **PNG-Dateien mit transparentem Hintergrund** sein und die Person darin sollte der in der UDP-Sequenz angegebenen Person so ähnlich wie möglich sein. Bitte versuchen Sie es mit MMD2UDP, um Ihre eigene UDP-Sequenz zu generieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GsjeMC0DD2TP"
      },
      "outputs": [],
      "source": [
        "#@title Download the sample UDPs and Character Sheet. Skip if you want to upload your own\n",
        "\n",
        "#@markdown Click start to download sample character sheets and UDP sequences.\n",
        "\n",
        "#@markdown If you choose `self_defined` in the last step, please upload your own UDP sequences or pose image sequences to `/content/CoNR/poses/`,  and upload your own character sheets to `/content/CoNR/character_sheet/character/`.\n",
        "\n",
        "\n",
        "!rm *.zip.*\n",
        "!rm -r character_sheet/\n",
        "!rm -r poses/\n",
        "!mkdir character_sheet/\n",
        "if character == 'short_hair':\n",
        "  !curl -O -J -L  https://github.com/transpchan/Live3D-v2/releases/download/samples/short_hair_images.zip\n",
        "  !unzip -j  short_hair_images.zip -x '__MACOSX/*'  -d character_sheet/character/ \n",
        "elif character == 'double_ponytail':\n",
        "  !curl -O -J -L  https://github.com/transpchan/Live3D-v2/releases/download/samples/double_ponytail_images.zip\n",
        "  !unzip -j  double_ponytail_images.zip -x '__MACOSX/*' -d character_sheet/character/\n",
        "else:\n",
        "  print(\"Please upload your character sheets to /content/CoNR/character_sheet/ \")\n",
        "if character == 'short_hair':\n",
        "  !curl -O -J -L  https://github.com/transpchan/Live3D-v2/releases/download/samples/short_hair.zip\n",
        "  !unzip -j  short_hair.zip -d poses/\n",
        "elif character == 'double_ponytail':\n",
        "  !curl -O -J -L  https://github.com/transpchan/Live3D-v2/releases/download/samples/double_ponytail.zip\n",
        "  !unzip -j double_ponytail.zip -d poses/ \n",
        "else:\n",
        "  print(\"Please upload your UDP sequences or poses images to /content/CoNR/poses/ .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "slg-lLN6KQyr"
      },
      "outputs": [],
      "source": [
        "#@title Show all character sheets\n",
        "from IPython.display import Image,display\n",
        "from pathlib import Path\n",
        "path ='./character_sheet/'\n",
        "imgs = []\n",
        "for file in Path(path).rglob('*.[PpWw][NnEe][GgBb]*'):\n",
        "          imgs.append(Image(filename=str(file), width=200))\n",
        "          \n",
        "print(\"Num of character sheets:\", len(imgs))\n",
        "display(*imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0EJwD5KTj6xL"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Install MMD2UDP dependencies\n",
        "%%capture\n",
        "!apt install ca-certificates \\\n",
        "software-properties-common \\\n",
        "&& add-apt-repository -y ppa:savoury1/ffmpeg4 >/dev/null \\\n",
        "&& add-apt-repository -y ppa:savoury1/display >/dev/null\n",
        "!apt install ffmpeg wget curl\n",
        "%cd /content/Live3D-v2\n",
        "!wget https://github.com/KurisuMakise004/MMD2UDP/raw/main/MMD2UDP_linux.7z\n",
        "!7z x MMD2UDP_linux.7z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkI0_7BxOS0N",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) MMD to UDP\n",
        "#@markdown Upload your model.zip, motion.vmd, and optionally camera.vmd file to Live3D-v2 folder using the file browser on the left. \n",
        "\n",
        "#@markdown model.zip must be a ZIP archive with with all your MMD files (your_model.pmx/vrm, and textures).\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "path = \"/content/Live3D-v2/poses\"\n",
        "\n",
        "if os.path.exists(path):\n",
        "    # delete contents of the directory\n",
        "    for file_name in os.listdir(path):\n",
        "        file_path = os.path.join(path, file_name)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                os.unlink(file_path)\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
        "else:\n",
        "    # create the directory\n",
        "    os.makedirs(path)\n",
        "!cp model.zip ./udp/\n",
        "!cp motion.vmd ./udp/\n",
        "# !cp camera.vmd ./udp/ disregard camera.vmd for now.\n",
        "!cd ./udp/ && chmod +x ./udp && ./udp \n",
        "!mv /content/Live3D-v2/udp/output/* /content/Live3D-v2/poses "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYF_YbZKkK2t",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Run UDP Detector\n",
        "#@markdown This additional demo will show the results by running the udp detector on the character sheet. If you want to run it on the pose sequence, you need to change the code in train.py\n",
        "# !pip install open3d\n",
        "# !mkdir results\n",
        "# !python3 train.py --mode=test \\\n",
        "# --test_input_poses_images=./myposes/ \\\n",
        "# --test_output_dir=./results/ \\\n",
        "# --test_checkpoint_dir=./weights/  \\\n",
        "# --test_output_udp=True \\\n",
        "# --test_output_video=False \\\n",
        "# --test_pose_use_parser_udp=True \\\n",
        "# --test_output_udp=True\n",
        "!pip install open3d\n",
        "!mkdir results\n",
        "!python3 train.py --mode=test \\\n",
        "--test_input_poses_images=./character_sheet/character/ \\\n",
        "--test_input_person_images=./character_sheet/ \\\n",
        "--test_output_dir=./results/ \\\n",
        "--test_checkpoint_dir=./weights/  \\\n",
        "--test_output_udp=True \\\n",
        "--test_output_video=False \\\n",
        "--test_pose_use_parser_udp=True\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0ctbV9g1GFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YUvUMo2-l0IW"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title (Optional) Visualize UDP detection results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import numpy as np\n",
        "    import open3d as o3d\n",
        "\n",
        "    npz = np.load('./results/udp_0.npz', allow_pickle=True)\n",
        "    print(\"img count:\", npz[\"udp\"].shape[0])\n",
        "    a = np.moveaxis(npz[\"udp\"][:, :, :, :], [2, 3], [0, 1]).reshape(-1, 4)\n",
        "    img = np.moveaxis(npz[\"img\"][:, :, :, :], [2, 3], [0, 1]).reshape(-1, 3)\n",
        "\n",
        "    occulusion = (a[:, 3] > 0.90)\n",
        "\n",
        "    xyz = a[occulusion, 0:3]\n",
        "    rgb = img[occulusion, 0:3]\n",
        "    print(\"points:\", xyz.shape[0])\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(xyz*[0.7, 0.25, 1])\n",
        "\n",
        "    pcd.colors = o3d.utility.Vector3dVector(rgb)\n",
        "    pcd.estimate_normals()\n",
        "    pcd.orient_normals_consistent_tangent_plane(1)\n",
        "\n",
        "    pcd2 = o3d.geometry.PointCloud()\n",
        "    pcd2.points = o3d.utility.Vector3dVector([\n",
        "        [0, 0, 0],\n",
        "        [1, 0, 0],\n",
        "        [0, 1, 0],\n",
        "        [1, 1, 0],\n",
        "        [0, 0, 1],\n",
        "        [1, 0, 1],\n",
        "        [0, 1, 1],\n",
        "        [1, 1, 1],\n",
        "    ])\n",
        "\n",
        "    pcd2.paint_uniform_color([0.5, 0.5, 0.5])\n",
        "    o3d.visualization.draw_plotly([pcd, pcd2])\n",
        "    if False:\n",
        "        print(\"Displaying pointcloud ...\")\n",
        "        o3d.visualization.draw([pcd])\n",
        "    o3d.io.write_point_cloud(\"./pointcloud.ply\", pcd)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC17TkyYEFIY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run Video Generation\n",
        "#@markdown For sample data, this process may take about 40 minutes. You can stop earlier to get an shorter result (by clicking stop on the left).\n",
        "!mkdir results\n",
        "!python3 train.py --mode=test \\\n",
        "--test_input_poses_images=./poses/ \\\n",
        "--test_input_person_images=./character_sheet/ \\\n",
        "--test_output_dir=./results/ \\\n",
        "--test_checkpoint_dir=./weights/ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqcBM7lL9eZk"
      },
      "outputs": [],
      "source": [
        "#@title Convert video format for display\n",
        "#@markdown `output.mp4` is the output with black background.  `output_adobe_premiere.mov` is the output with transparent background.\n",
        "!ffmpeg -r 30 -y -i /content/Live3D-v2/results/%d.png -c:v qtrle output_adobe_premiere.mov \n",
        "!ffmpeg -r 30 -y -i /content/Live3D-v2/results/%d.png  -c:v libx264 -strict -2 -pix_fmt yuv420p   output.mp4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NfzPg6ESEhYE"
      },
      "outputs": [],
      "source": [
        "#@title Play the generated video!\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        " \n",
        "def show_video(video_path, video_width = 600):\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        " \n",
        "show_video('output.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Optional) Real-ESRGAN Upscaler Dependencies\n",
        "%%capture\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "!pip install basicsr\n",
        "# facexlib and gfpgan are for face enhancement\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P weights\n",
        "%cd /content/Live3D-v2"
      ],
      "metadata": {
        "id": "FzQ1vncE1NfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the Upscaler\n",
        "%cd /content/Live3D-v2/Real-ESRGAN\n",
        "import os\n",
        "import glob\n",
        "\n",
        "path = \"/content/Live3D-v2/results\"  # replace with the path to your directory\n",
        "png_files = glob.glob(os.path.join(path, \"*.png\"))\n",
        "\n",
        "for png_file in png_files:\n",
        "  !python inference_realesrgan.py -n RealESRGAN_x4plus_anime_6B -i {png_file} -o /content/Live3D-v2/upscaled\n",
        "%cd /content/Live3D-v2"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HzQbjmqv2OgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rMyxlpqyEcsW"
      },
      "outputs": [],
      "source": [
        "#@title Convert video format for display with upscaled images\n",
        "#@markdown `output.mp4` is the output with black background.  `output_adobe_premiere.mov` is the output with transparent background.\n",
        "!ffmpeg -r 30 -y -i /content/Live3D-v2/upscaled/%d_out.png -c:v qtrle output_adobe_premiere.mov \n",
        "!ffmpeg -r 30 -y -i /content/Live3D-v2/upscaled/%d_out.png  -c:v libx264 -strict -2 -pix_fmt yuv420p   output.mp4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FkqQ1LEK9xbi"
      },
      "outputs": [],
      "source": [
        "#@title Play the generated upscaled video!\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        " \n",
        "def show_video(video_path, video_width = 600):\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        " \n",
        "show_video('output.mp4')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}